{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a brief tutorial on how to use LLAMBO for your own black-box functions. First, please make sure you follow the steps in ```README``` to set up your environment.\n",
    "\n",
    "In this script, we will provide:\n",
    "1. Overview of ```LLAMBO``` class,\n",
    "2. Tutorial of how you can run ```LLAMBO``` on your black-box function by\n",
    "    - Providing ```task_context```, which semantically describes the problem sapce\n",
    "    - Providing ```init_f``` (initialization function with customizable initialization strategy) and ```bbox_eval_f``` (function to evaluate proposed point on the black-box function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of LLAMBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "llambo_opt = LLAMBO(\n",
    "    task_context: dict,         # dictionary describing task\n",
    "    sm_mode,                    # either 'generative' or 'discriminative', for generative or discriminative surrogate model\n",
    "    n_candidates,               # number of candidate points to sample at each iteration\n",
    "    n_templates,                # number of different prompts (or templates) used for LLM queries\n",
    "    n_gens,                     # number of generations for LLM, set at 5\n",
    "    alpha,                      # alpha for candidate point sampler, recommended to be -0.2\n",
    "    n_initial_samples,          # number of initialization points to evaluate\n",
    "    n_trials,                   # number of trials to run,\n",
    "    init_f,                     # function to generate initial configurations\n",
    "    bbox_eval_f,                # bbox function to evaluate a point\n",
    "    chat_engine,                # LLM chat engine\n",
    "    top_pct=None,               # only used for generative SM, top percentage of points to consider for generative SM\n",
    "    use_input_warping=False,    # whether to use input warping\n",
    "    prompt_setting=None,        # ablation on prompt design, either 'full_context' or 'partial_context' or 'no_context' (only used for ablation experiments)\n",
    "    shuffle_features=False      # whether to shuffle features in prompt generation (only used for ablation experiments)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use LLAMBO optimizer, you would need to provide two key components:\n",
    "- ```task_context```: contextual information about the optimization problem that is used to construct prompts\n",
    "- ```init_f```: function to generate ```n_initial_samples``` (e.g. 5) initial points used to initialize the BO search process\n",
    "- ```bbox_eval_f```: bbox function that evaluates a proposed point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **task_context**: Here is an example task_context, that is automatically extracted for Bayesmark task: [```RandomForest``` (model), ```breast``` (dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "task_context = {\n",
    "    'model': 'RandomForest', \n",
    "    'task': 'classification', \n",
    "    'tot_feats': 30, \n",
    "    'cat_feats': 0, \n",
    "    'num_feats': 30, \n",
    "    'n_classes': 2, \n",
    "    'metric': 'accuracy', \n",
    "    'lower_is_better': False, \n",
    "    'num_samples': 455, \n",
    "    'hyperparameter_constraints': {\n",
    "        'max_depth': ['int', 'linear', [1, 15]],        # [type, transform, [min_value, max_value]]\n",
    "        'max_features': ['float', 'logit', [0.01, 0.99]], \n",
    "        'min_impurity_decrease': ['float', 'linear', [0.0, 0.5]], \n",
    "        'min_samples_leaf': ['float', 'logit', [0.01, 0.49]], \n",
    "        'min_samples_split': ['float', 'logit', [0.01, 0.99]], \n",
    "        'min_weight_fraction_leaf': ['float', 'logit', [0.01, 0.49]]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **init_f**:\n",
    "```\n",
    "def init_f(n_samples: int):\n",
    "    '''\n",
    "    Generate initialization points for BO search\n",
    "    Args: n_samples (int)\n",
    "    Returns: init_configs (list of dictionaries, each dictionary is a point to be evaluated)\n",
    "    '''\n",
    "    return initial_samples\n",
    "```\n",
    "The initialization function should accept ```n_samples (int)```, which indicates the number of initial points to return, and returns them in a list of dictionaries, where each dictionary is a point to be evaluated.\n",
    "\n",
    "3. **bbox_eval_f**: \n",
    "\n",
    "```\n",
    "def bbox_eval_f(point_to_evaluate: dictionary):\n",
    "    '''\n",
    "    Evaluate a single point on bbox function\n",
    "    Args: point_to_evaluate (dict), dictionary containing point to be evaluated\n",
    "    Returns: (point_to_evaluate, f_vals) (dict, dict)\n",
    "             point_to_evaluate (dict) is the point evaluated\n",
    "             f_vals (dict) is a dictionary that can track an arbitrary number of metrics, but must contain 'score' which is what LLAMBO optimizer tries to optimize by default\n",
    "\n",
    "    Example f_vals:\n",
    "    f_vals = {\n",
    "        'score': float,                     -> 'score' is what the LLAMBO optimizer tries to optimize\n",
    "        'generalization_score': float,\n",
    "        'acc': float,\n",
    "        ...\n",
    "        'f1': float\n",
    "    }\n",
    "    '''\n",
    "```\n",
    "The black-box evaluation function should accept ```point_to-evaluate (dict)``` which is a dictionary containing the point to be evaluated, and returns this point and a dictionary of evaluation results.\n",
    "\n",
    "\n",
    "Below is an example of a class written to run Bayesmark BO tasks, which contains two functions ```generate_initialization``` (init_f) and ```evalaute_point``` (bbox_eval_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class BayesmarkExpRunner:\n",
    "    def __init__(self, task_context, dataset, seed):\n",
    "        self.seed = seed\n",
    "        self.model = task_context['model']\n",
    "        self.task = task_context['task']\n",
    "        self.metric = task_context['metric']\n",
    "        self.dataset = dataset\n",
    "        self.hyperparameter_constraints = task_context['hyperparameter_constraints']\n",
    "        self.bbox_func = get_bayesmark_func(self.model, self.task, dataset['test_y'])\n",
    "    \n",
    "    def generate_initialization(self, n_samples):\n",
    "        '''\n",
    "        Generate initialization points for BO search\n",
    "        Args: n_samples (int)\n",
    "        Returns: init_configs (list of dictionaries, each dictionary is a point to be evaluated)\n",
    "        '''\n",
    "\n",
    "        # Read from fixed initialization points (all baselines see same init points)\n",
    "        init_configs = pd.read_json(f'bayesmark/configs/{self.model}/{self.seed}.json').head(n_samples)\n",
    "        init_configs = init_configs.to_dict(orient='records')\n",
    "\n",
    "        assert len(init_configs) == n_samples\n",
    "\n",
    "        return init_configs\n",
    "        \n",
    "    def evaluate_point(self, candidate_config):\n",
    "        '''\n",
    "        Evaluate a single point on bbox\n",
    "        Args: candidate_config (dict), dictionary containing point to be evaluated\n",
    "        Returns: (dict, dict), first dictionary is candidate_config (the evaluated point), second dictionary is fvals (the evaluation results)\n",
    "        '''\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = self.dataset['train_x'], self.dataset['test_x'], self.dataset['train_y'], self.dataset['test_y']\n",
    "\n",
    "        for hyperparam, value in candidate_config.items():\n",
    "            if self.hyperparameter_constraints[hyperparam][0] == 'int':\n",
    "                candidate_config[hyperparam] = int(value)\n",
    "\n",
    "        if self.task == 'regression':\n",
    "            mean_ = np.mean(y_train)\n",
    "            std_ = np.std(y_train)\n",
    "            y_train = (y_train - mean_) / std_\n",
    "            y_test = (y_test - mean_) / std_\n",
    "\n",
    "        model = self.bbox_func(**candidate_config)\n",
    "        scorer = get_scorer(self.metric)\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore', category=UserWarning)\n",
    "            S = cross_val_score(model, X_train, y_train, scoring=scorer, cv=5)\n",
    "        cv_score = np.mean(S)\n",
    "        \n",
    "        model = self.bbox_func(**candidate_config)  \n",
    "        model.fit(X_train, y_train)\n",
    "        generalization_score = scorer(model, X_test, y_test)\n",
    "\n",
    "        if self.metric == 'neg_mean_squared_error':\n",
    "            cv_score = -cv_score\n",
    "            generalization_score = -generalization_score\n",
    "\n",
    "        return candidate_config, {'score': cv_score, 'generalization_score': generalization_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it Together!\n",
    "\n",
    "After preparing ```task_context```, your search initialization function (```init_f```), and your black box function (```bbox_eval_f```), you can run LLAMBO optimization with a few lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dataset = 'breast'\n",
    "seed = 0\n",
    "chat_engine = # LLM Chat Engine, currently our code only supports OpenAI LLM API\n",
    "\n",
    "# load data\n",
    "pickle_fpath = f'bayesmark/data/{dataset}.pickle'\n",
    "with open(pickle_fpath, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# instantiate BayesmarkExpRunner\n",
    "benchmark = BayesmarkExpRunner(task_context, data, seed)\n",
    "\n",
    "# instantiate LLAMBO\n",
    "llambo = LLAMBO(task_context, sm_mode='discriminative', n_candidates=10, n_templates=2, n_gens=10, \n",
    "                alpha=0.1, n_initial_samples=5, n_trials=25, \n",
    "                init_f=benchmark.generate_initialization,\n",
    "                bbox_eval_f=benchmark.evaluate_point, \n",
    "                chat_engine=chat_engine)\n",
    "llambo.seed = seed\n",
    "\n",
    "# run optimization\n",
    "configs, fvals = llambo.optimize()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
